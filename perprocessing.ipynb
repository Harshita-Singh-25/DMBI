{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyryD3yl1GCoV14R5iOEuG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harshita-Singh-25/DMBI/blob/main/perprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wo55fQZuiNsh",
        "outputId": "3d153210-094c-4ae2-ec2b-d3d7a60e69c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "ORIGINAL DATA\n",
            "==================================================\n",
            "Shape: (150, 5)\n",
            "   sepal_length  sepal_width  petal_length  petal_width Species\n",
            "0           5.1          3.5           1.4          0.2  setosa\n",
            "1           4.9          3.0           1.4          0.2  setosa\n",
            "2           4.7          3.2           1.3          0.2  setosa\n",
            "3           4.6          3.1           1.5          0.2  setosa\n",
            "4           5.0          3.6           1.4          0.2  setosa\n",
            "\n",
            "Missing Values:\n",
            "sepal_length    0\n",
            "sepal_width     0\n",
            "petal_length    0\n",
            "petal_width     0\n",
            "Species         0\n",
            "dtype: int64\n",
            "\n",
            " Missing values handled\n",
            " Removed 1 duplicate rows\n",
            " Removed 4 outliers from sepal_width\n",
            "Encoded Species\n",
            "\n",
            "✅ Feature scaling completed\n",
            "\n",
            "==================================================\n",
            "PREPROCESSED DATA\n",
            "==================================================\n",
            "Shape: (145, 6)\n",
            "   sepal_length  sepal_width  petal_length  petal_width Species  \\\n",
            "0     -0.907877     1.152206     -1.366548    -1.341462  setosa   \n",
            "1     -1.147662    -0.107748     -1.366548    -1.341462  setosa   \n",
            "2     -1.387447     0.396234     -1.423536    -1.341462  setosa   \n",
            "3     -1.507340     0.144243     -1.309559    -1.341462  setosa   \n",
            "4     -1.027769     1.404196     -1.366548    -1.341462  setosa   \n",
            "\n",
            "   Species_Encoded  \n",
            "0                0  \n",
            "1                0  \n",
            "2                0  \n",
            "3                0  \n",
            "4                0  \n",
            "\n",
            "Data Types:\n",
            "sepal_length       float64\n",
            "sepal_width        float64\n",
            "petal_length       float64\n",
            "petal_width        float64\n",
            "Species             object\n",
            "Species_Encoded      int64\n",
            "dtype: object\n",
            "\n",
            "✅ Data Preprocessing Completed Successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "######\n",
        "import seaborn as sns\n",
        "df = sns.load_dataset('iris')\n",
        "# The seaborn version has different column names, so let's rename one to match your original script's logic (which looks for 'Species').\n",
        "df.rename(columns={'species': 'Species'}, inplace=True)\n",
        "##########\n",
        "\n",
        "\n",
        "# Load dataset stored in the Files in the left side bar:\n",
        "#df = pd.read_csv('Iris.csv')  # Change filename as needed\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"ORIGINAL DATA\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(df.head())\n",
        "print(f\"\\nMissing Values:\\n{df.isnull().sum()}\")\n",
        "\n",
        "# ========================================\n",
        "# 1. HANDLE MISSING VALUES\n",
        "# ========================================\n",
        "\n",
        "# Drop rows where all values are missing\n",
        "df = df.dropna(how='all')\n",
        "\n",
        "# For numeric columns: fill with median\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "df[numeric_cols] = imputer.fit_transform(df[numeric_cols])\n",
        "\n",
        "# For categorical columns: fill with mode\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "for col in categorical_cols:\n",
        "    if df[col].isnull().sum() > 0:\n",
        "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "\n",
        "print(\"\\n Missing values handled\")\n",
        "\n",
        "# ========================================\n",
        "# 2. REMOVE DUPLICATES\n",
        "# ========================================\n",
        "\n",
        "original_rows = len(df)\n",
        "df = df.drop_duplicates()\n",
        "print(f\" Removed {original_rows - len(df)} duplicate rows\")\n",
        "\n",
        "# ========================================\n",
        "# 3. HANDLE OUTLIERS (IQR Method)\n",
        "# ========================================\n",
        "\n",
        "def remove_outliers(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "    return df[(df[column] >= lower) & (df[column] <= upper)]\n",
        "\n",
        "# Apply to numeric columns (skip ID columns)\n",
        "for col in numeric_cols:\n",
        "    if col not in ['Id', 'id', 'ID']:\n",
        "        before = len(df)\n",
        "        df = remove_outliers(df, col)\n",
        "        removed = before - len(df)\n",
        "        if removed > 0:\n",
        "            print(f\" Removed {removed} outliers from {col}\")\n",
        "\n",
        "# ========================================\n",
        "# 4. ENCODE CATEGORICAL VARIABLES\n",
        "# ========================================\n",
        "\n",
        "le = LabelEncoder()\n",
        "for col in categorical_cols:\n",
        "    if col != 'Id':  # Skip ID columns\n",
        "        df[col + '_Encoded'] = le.fit_transform(df[col])\n",
        "        print(f\"Encoded {col}\")\n",
        "\n",
        "# ========================================\n",
        "# 5. FEATURE SCALING (Standardization)\n",
        "# ========================================\n",
        "\n",
        "# Select numeric features for scaling (exclude encoded columns)\n",
        "features_to_scale = [col for col in numeric_cols if col not in ['Id', 'id', 'ID']]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df[features_to_scale] = scaler.fit_transform(df[features_to_scale])\n",
        "\n",
        "print(\"\\n✅ Feature scaling completed\")\n",
        "\n",
        "# ========================================\n",
        "# FINAL OUTPUT\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PREPROCESSED DATA\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(df.head())\n",
        "print(f\"\\nData Types:\\n{df.dtypes}\")\n",
        "\n",
        "print(\"\\n✅ Data Preprocessing Completed Successfully!\")"
      ]
    }
  ]
}